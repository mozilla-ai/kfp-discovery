name: osm-inference
description: Run inference on provided map tiles.

inputs:
- {name: model_dir, type: Artifact, description: The local directory where the model is downloaded.}
- {name: model_name, type: String, default: model.pt, description: The name of the YOLO model to be used for inference.}
- {name: latitude, type: Float, description: The latitude of the center of the map tile.}
- {name: longitude, type: Float, description: The longitude of the center of the map tile.}
- {name: batch_size, type: Integer, default: 32, description: The batch size for inference.}
- {name: mapbox_token, type: String, description: The Mapbox token for accessing map tiles.}

outputs:
- {name: output_dir, type: Artifact, description: The local directory where the results (.osm) of inference are stored.}

metadata:
  annotations:
    resources/cpu: "8"
    resources/memory: "32Gi"
    resources/accelerator: "1"
    resources/accelerator-type: "nvidia.com/gpu"
    tags: maps, satellite, vision, osm, inference, mapbox

implementation:
  container:
    image: mzdotai/workflow-components-osm-inference:latest
    command: [python]
    args: [
      run_inference.py,
      --model-dir, {inputPath: model_dir},
      --model-name, {inputValue: model_name},
      --latitude, {inputValue: latitude},
      --longitude, {inputValue: longitude},
      --batch-size, {inputValue: batch_size},
      --mapbox-token, {inputValue: mapbox_token},
      --output-dir, {outputPath: output_dir}
    ]
